---
title: "p8105_hw3_jl5788"
author: "Jie Liu"
date: "10/18/2021"
output: github_document
---


# Problem 1

```{r,include=FALSE}
library(tidyverse)
library(p8105.datasets)
```

```{r}
data("instacart")
instacart =
  instacart %>% 
  as_tibble(instacart)
```

#### General Description of Dataset

This dataset contains `r nrow(instacart)` rows and `r ncol(instacart)` columns, with each row representing a product from an instacart order. Total 15 Variables include identifier for order, product, user and aisle. The dataset includes name-specific variables like the name of the department(e.g. daily eggs, produce), aisle(e.g. yogurt, fresh vegetables) and product(e.g. Bulgarian Yogurt, Asparagus). Besides, there are some order-level variables, describing the order sequence number for one user, the day of the week on which the order was placed and the hour of the day on which the order was placed.The dataset also contains a variable (named "reordered") describing that whether the product has been ordered by this user in the past.

In total, there are `r instacart %>% select(product_name) %>% distinct %>% count` kind of products from `r instacart %>% select(order_id) %>% distinct %>% count` orders by `r instacart %>% select(user_id) %>% distinct %>% count` users.

#### Below is a 134 x 2 tibble showing the number of items ordered from aisle and sorting in descending order

There are 134 aisles, with fresh vegetables ranking No.1 of the items ordered.
```{r}
instacart %>% 
  count(aisle) %>% 
  arrange(desc(n))

```


#### Below is a plot that shows the number of items ordered in each aisle(items whose number is <=10000 are excluded)

```{r}
instacart %>% 
  count(aisle) %>% 
  filter(n>10000) %>% 
  mutate(aisle=fct_reorder(aisle,n)) %>% 
  arrange(desc(n)) %>% 
  ggplot(aes(x = aisle, y = n)) + geom_point() +labs(x = "Aisle",y= "Number",title = "Number of items ardered in each aisle") + theme(axis.text.x = element_text(angle = 50, hjust = 1.2))
```

#### Below is a table showing the three most poplular items in each of the aisles "baking ingredients”, “dog food care”, and “packaged vegetables fruits”.

```{r}
instacart %>% 
  filter(aisle %in% c( "baking ingredients","dog food care", "packaged vegetables fruits")) %>% 
  group_by(aisle) %>% 
  select(aisle,everything()) %>% 
  count(product_name) %>% 
  mutate(rank = min_rank(desc(n))) %>% 
  filter( rank<4 ) %>% 
  arrange(desc(n)) %>% 
  knitr::kable()
```

#### A table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week 

```{r}
instacart %>% 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  group_by(product_name,order_dow) %>% 
  summarize(mean_hour = mean(order_hour_of_day)) %>% 
  spread(key=order_dow, value= mean_hour) %>% 
  knitr::kable(digits = 2)

```

# Problem 2

#### Data cleaning
```{r}
data("brfss_smart2010")
brfss =
  brfss_smart2010 %>% 
  as_tibble(brfss_smart2010) %>% 
  janitor::clean_names() %>% 
  filter(topic %in% c("Overall Health")) %>% 
  filter(response %in% c("Excellent", "Poor","Very good", "Good","Fair")) %>% 
  mutate(response = fct_relevel(response, c("Poor", "Fair", "Good", "Very good", "Excellent")))
  
```
#### In 2002, which states were observed at 7 or more locations? What about in 2010?
According to filtered data below,in 2002, there are 6 states observed at 7 or more locations, which are CT, FL, MA, NC, NJ, PA.
In 2010, there are 14 states observed at 7 or more locations, which are CA, CO, FL, MA, MD, NC, NE, NJ, NY, OH.
```{r}
location_2002 =
  brfss %>% 
  filter(year == 2002) %>% 
  group_by(locationabbr,locationdesc) %>% 
  summarize(n_obs =n()) %>% 
  summarize(n_obs =n()) %>% 
  filter(n_obs >=7)

location_2010 =
  brfss %>% 
  filter(year == 2010) %>% 
  group_by(locationabbr,locationdesc) %>% 
  summarize(n_obs =n()) %>% 
  summarize(n_obs =n()) %>% 
  filter(n_obs >=7)
```

#### Construct a dataset

```{r}
Excellent_dataset =
  brfss %>% 
  filter(response== "Excellent") %>% 
  group_by(locationabbr,locationdesc) %>% 
  mutate(mean=mean(data_value)) %>% 
  select(response,year,locationabbr,locationdesc,mean) 

excell_df =
  brfss %>% 
  filter(response == "Excellent") %>% 
  select(year, locationabbr, data_value) %>% 
  group_by(year, locationabbr) %>% 
  summarize(mean = mean(data_value))
  
excell_df %>% 
  group_by(locationabbr) %>% 
  ggplot(aes(x = year, y = mean, color = locationabbr)) +
  geom_line(alpha = 0.5) +
  labs(
    title = "The mean data value of state vs year",
    y = "Mean data value", 
    x = "Year",
  ) +
  scale_colour_hue("State abb") +
  theme(legend.key.size = unit(0.02,'cm')) +
  theme(legend.key.width = unit(0.05,'cm'))

```

#### Make a two-panel plot showing
```{r}
brfss %>%
  filter(locationabbr == "NY", year %in% c(2006, 2010)) %>%
  ggplot(aes(x = data_value, color = as.character(year))) +
  geom_density() +
  facet_grid(. ~ year) + 
  labs(title = "Distribution of data value of responses for 2006 and 2010 in NY State",
       x = "Data value of responses",
       y = "Density of data value")

```


# Problem 3

#### Load,tidy,and otherwise wrangle the data

```{r}
accelerometer_df=
  read_csv("data/accel_data.csv") %>% 
  janitor::clean_names() %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = "minute",
    names_prefix = "activity_",
    values_to = "activity_count") %>% 
  mutate(minute=as.numeric(minute),day=factor(day),day=fct_relevel(day,"Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"),weekend=as.numeric(day%in% c("Saturday","Sunday")),weekend=recode(weekend,'1'="weekend",'0'="weekday")) 


accelerometer_df=
  read_csv("data/accel_data.csv") %>% 
  janitor::clean_names() %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = "minute",
    names_prefix = "activity_",
    values_to = "activity_count") %>% 
  mutate(minute=as.numeric(minute),day=factor(day),day=fct_relevel(day,"Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"),weekend=as.numeric(day))
```
This cleaned dataset concerning  accelerometer data collected on a 63 year-old male with BMI 25 has `r nrow(accelerometer_df)` rows and `r ncol(accelerometer_df)` columns.Altogether,it has 5 variables,which are week,day,minute and activity counts.


#### Table of activity number in specific day and week 

AS shown in dataframe and table beblow,from the first week to the forth week, every Sunday's activity keeps decreasing. However, in other day of 4 weeks, it is hard to find any trend and establish connection between time and activity number. Besides, two saturdays's activity data are extremely unusual and much lower than other data, which could be resulted from data collection error.

```{r}
accelerometer_df %>% 
  group_by(day,week) %>% 
  summarize(total_act=sum(activity_count)) %>% 
  pivot_wider(
    names_from = day,
    values_from = total_act) %>% 
  select(week,Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday) %>% 
  knitr::kable()
```

#### Graphic of 24-hour activity over the course of the day

```{r}
accelerometer_df %>% 
  ggplot(aes(x=minute,y=activity_count,group=day_id,color=day))+
  geom_line(alpha=.3)+
  geom_smooth(aes(group = day), se =FALSE)+
  labs(title = "24_Hour Activity Over the Course of the Day" ,x= "Time", y="Activity Count") +
  scale_x_continuous(
    breaks = c(seq(from=0,to=1440,by=120)),
    labels = c(seq(from=0,to=1440,by=120))
  )
```

As shown in the plot, at the weekend, people tend to engage in the activity in the morning.On Friday, during the time before midnight, activity count are higher than other time,which means people are more likely to engage in the activity during the time before midnight on Friday. Compared those seven curves, it seems no other special conclusion could been drawn based on information provided. 



